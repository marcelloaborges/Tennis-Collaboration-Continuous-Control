{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration Continuous Control - Multi-Agent Deep Deterministic Policy Gradienty (MADDPG)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# environment configuration\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\", no_graphics=False, worker_id=1)\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Number of actions: 4\n",
      "States look like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "States have length: 33\n"
     ]
    }
   ],
   "source": [
    "# environment information\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# number of agents in the environment\n",
    "n_agents = len(env_info.agents)\n",
    "print('Number of agents:', n_agents)\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "import copy\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import Actor, Critic\n",
    "\n",
    "\n",
    "\n",
    "class Agent():        \n",
    "    def __init__(self, \n",
    "        device,\n",
    "        state_size, n_agents, action_size, random_seed,\n",
    "        buffer_size, batch_size, gamma, TAU, lr_actor, lr_critic, weight_decay,\n",
    "        checkpoint_folder = './'):        \n",
    "\n",
    "        self.DEVICE = device\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.n_agents = n_agents\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.BUFFER_SIZE = buffer_size\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.GAMMA = gamma\n",
    "        self.TAU = TAU\n",
    "        self.LR_ACTOR = lr_actor\n",
    "        self.LR_CRITIC = lr_critic\n",
    "        self.WEIGHT_DECAY = weight_decay\n",
    "\n",
    "        self.CHECKPOINT_FOLDER = checkpoint_folder\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=self.LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=self.LR_CRITIC, weight_decay=self.WEIGHT_DECAY)\n",
    "\n",
    "        if os.path.isfile(self.CHECKPOINT_FOLDER + 'checkpoint_actor.pth') and os.path.isfile(self.CHECKPOINT_FOLDER + 'checkpoint_critic.pth'):\n",
    "            self.actor_local.load_state_dict(torch.load(self.CHECKPOINT_FOLDER + 'checkpoint_actor.pth'))\n",
    "            self.actor_target.load_state_dict(torch.load(self.CHECKPOINT_FOLDER + 'checkpoint_actor.pth'))\n",
    "\n",
    "            self.critic_local.load_state_dict(torch.load(self.CHECKPOINT_FOLDER + 'checkpoint_critic.pth'))\n",
    "            self.critic_target.load_state_dict(torch.load(self.CHECKPOINT_FOLDER + 'checkpoint_critic.pth'))\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((n_agents, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(device, action_size, self.BUFFER_SIZE, self.BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for i in range(self.n_agents):\n",
    "            self.memory.add(state[i,:], action[i,:], reward[i], next_state[i,:], done[i])\n",
    "\n",
    "        # Learn, if enough samples are available in memory        \n",
    "        if len(self.memory) > self.BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.DEVICE)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()        \n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (self.GAMMA * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local, self.actor_target)   \n",
    "           \n",
    "        self.actor_loss = actor_loss.data\n",
    "        self.critic_loss = critic_loss.data        \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        tau = self.TAU\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def checkpoint(self):\n",
    "        torch.save(self.actor_local.state_dict(), self.CHECKPOINT_FOLDER + 'checkpoint_actor.pth')      \n",
    "        torch.save(self.critic_local.state_dict(), self.CHECKPOINT_FOLDER + 'checkpoint_critic.pth')  \n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size = size        \n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state        \n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, device, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.DEVICE = device\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"        \n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.DEVICE)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "CHECKPOINT_FOLDER = './'\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "                DEVICE, \n",
    "                state_size, n_agents, action_size, 4, \n",
    "                BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR_ACTOR, LR_CRITIC, WEIGHT_DECAY,\n",
    "                CHECKPOINT_FOLDER\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: \t137 \tScore: \t36.24 \tAverage Score: \t30.30\n",
      "Environment solved in 137 episodes!\tAverage Score: 30.30\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ddpg_train():\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    n_episodes = 1000\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]            # reset the environment\n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()                                                # reset the agent noise\n",
    "        score = np.zeros(n_agents)\n",
    "        \n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "        \n",
    "            env_info = env.step( actions )[brain_name]               # send the action to the environment                            \n",
    "            next_states = env_info.vector_observations               # get the next state        \n",
    "            rewards = env_info.rewards                               # get the reward        \n",
    "            dones = env_info.local_done                              # see if episode has finished        \n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            score += rewards                                         # update the score\n",
    "        \n",
    "            states = next_states                                     # roll over the state to next time step        \n",
    "                                                        \n",
    "            if np.any( dones ):                                          # exit loop if episode finished        \n",
    "                break                                        \n",
    "\n",
    "        agent.checkpoint()\n",
    "\n",
    "        scores.append(np.mean(score))\n",
    "        scores_window.append(np.mean(score))\n",
    "\n",
    "        print('\\rEpisode: \\t{} \\tScore: \\t{:.2f} \\tAverage Score: \\t{:.2f}'.format(episode, np.mean(score), np.mean(scores_window)), end=\"\")  \n",
    "        \n",
    "        if np.mean(scores_window) >= 30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "            break    \n",
    "\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "# train the agent\n",
    "ddpg_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: \t0 \tScore: \t39.30\n",
      "Episode: \t1 \tScore: \t38.96\n",
      "Episode: \t2 \tScore: \t39.06\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "agent = Agent(\n",
    "                DEVICE, \n",
    "                state_size, n_agents, action_size, 4, \n",
    "                BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR_ACTOR, LR_CRITIC, WEIGHT_DECAY,\n",
    "                CHECKPOINT_FOLDER\n",
    "        )\n",
    "\n",
    "for episode in range(3):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]        \n",
    "    states = env_info.vector_observations       \n",
    "    score = np.zeros(n_agents)               \n",
    "    \n",
    "    while True:\n",
    "        actions = agent.act(states, add_noise=False)                    \n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]        \n",
    "        next_states = env_info.vector_observations     \n",
    "        rewards = env_info.rewards       \n",
    "        dones = env_info.local_done\n",
    "        score += rewards\n",
    "        states = next_states\n",
    "\n",
    "        if np.any(dones):                              \n",
    "            break\n",
    "\n",
    "    print('Episode: \\t{} \\tScore: \\t{:.2f}'.format(episode, np.mean(score)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
